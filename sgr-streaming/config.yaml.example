# SGR Research Agent - Configuration Template
# Production-ready configuration for Schema-Guided Reasoning
# Скопируйте этот файл в config.yaml и заполните своими API ключами

# OpenAI Configuration
openai:
  api_key: "your-openai-api-key-here"  # Обязательно: Ваш OpenAI API ключ или master_key LiteLLM
  base_url: ""                         # Опционально: Альтернативный URL (например, LiteLLM proxy http://localhost:8000/v1)
  model: "gpt-4o-mini"                 # Модель для использования (или имя из LiteLLM model_list, напр. sgr-gemma)
  max_tokens: 12000                    # Максимальное количество токенов (увеличено для полного JSON)
  temperature: 0.2                     # Температура генерации (понижена для лучшего structured output)

# Tavily Search Configuration
tavily:
  api_key: "your-tavily-api-key-here"  # Обязательно: Ваш Tavily API ключ

# Search Settings
search:
  max_results: 10                      # Максимальное количество результатов поиска

# Scraping Settings
scraping:
  enabled: false                       # Включить скрейпинг полного текста найденных страниц
  max_pages: 5                         # Максимум страниц для скрейпинга за поиск
  content_limit: 1500                  # Лимит символов полного контента на источник

# Execution Settings
execution:
  max_steps: 6                         # Максимальное количество шагов выполнения
  reports_dir: "reports"               # Директория для сохранения отчетов



# ============================================================================
# ИНСТРУКЦИИ ПО НАСТРОЙКЕ:
# ============================================================================
#
# 1. Получите OpenAI API ключ:
#    - Зайдите на https://platform.openai.com/api-keys
#    - Создайте новый API ключ
#    - Скопируйте его в поле openai.api_key
#
# 2. Получите Tavily API ключ:
#    - Зайдите на https://tavily.com
#    - Зарегистрируйтесь и получите API ключ
#    - Скопируйте его в поле tavily.api_key
#
# 3. Альтернативно, можете использовать переменные окружения:
#    export OPENAI_API_KEY="your-openai-key"
#    export TAVILY_API_KEY="your-tavily-key"
#
# 4. Создайте директорию для отчетов:
#    mkdir reports
#
# ============================================================================
# ============================================================================
# ЛОКАЛЬНЫЕ МОДЕЛИ (Local Models Setup):
# ============================================================================
#
# Для использования локальных моделей через Ollama:
#
# 1. Запустите Ollama:
#    ollama serve
#
# 2. Запустите LiteLLM proxy:
#    litellm --config proxy/litellm_config.yaml --host 0.0.0.0 --port 8000
#
# 3. (Опционально) Запустите airsroute gateway:
#    python -m uvicorn proxy.airsroute_gateway.app:app --reload --port 8010
#
# 4. Настройте конфигурацию:
#    openai:
#      api_key: "dev-key"                    # Должен совпадать с master_key в LiteLLM
#      base_url: "http://localhost:8000/v1"  # LiteLLM proxy URL
#      model: "sgr-gemma"                    # Имя из model_list в litellm_config.yaml
#      temperature: 0.1                     # Низкая температура для structured output
#      max_tokens: 16000                    # Больше токенов для сложных схем
#
# 5. Включите отладку JSON (опционально):
#    export SGR_DEBUG_JSON=1
#
# Рекомендуемые модели для structured output:
# - gemma2:27b (лучший результат)
# - llama3.1:70b (отличный результат)
# - gemma2:9b (хороший результат)
# - llama3.1:8b (приемлемый результат)
#
# ============================================================================