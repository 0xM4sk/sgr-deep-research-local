# ðŸ§  SGR Deep Research - Schema-Guided Reasoning System

https://github.com/user-attachments/assets/a5e34116-7853-43c2-ba93-2db811b8584a

Fully Local (<8B) Automated research system using Schema-Guided Reasoning (SGR).

Credits
- Originator: vakovalskii/sgr-deep-research

Current State (Local Stack)
- Fully local SGR via LiteLLM + Ollama is working with subâ€‘8B models (e.g., llama3â€‘8b, gemmaâ€‘7b/9b). The streaming app guides models to produce structured JSON reliably, then renders and saves Markdown reports.
- Airsroute gateway integration is workâ€‘inâ€‘progress (WIP) and optional.

Local Run (LiteLLM + Ollama)
- Install deps: `pip install -r sgr-streaming/requirements.txt`
- Start Ollama: `ollama serve` (ensure your target model is pulled, e.g., `ollama pull llama3:8b`)
- Start LiteLLM proxy: `litellm --config sgr-streaming/proxy/litellm_config.yaml --host 0.0.0.0 --port 8000`
- Configure `sgr-streaming/config.yaml`:
  - `openai.api_key: dev-key`
  - `openai.base_url: http://localhost:8000/v1`
  - `openai.model: sgr-gemma` (or the model alias from `litellm_config.yaml`)
- Run streaming app: `python sgr-streaming/sgr_streaming.py`

Notes
- Strictness knobs for report saving are configurable in `execution` (e.g., `strict_report_quality`, `min_report_words`).
- Airsroute gateway and dashboard exist but are optional; they will be documented when stable.

## ðŸ“ Project Structure

```
sgr-deep-research/
â”œâ”€â”€ sgr-classic/          # ðŸ” Classic SGR version
â”‚   â”œâ”€â”€ sgr-deep-research.py
â”‚   â”œâ”€â”€ scraping.py
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ sgr-streaming/        # ðŸš€ Enhanced streaming version
â”‚   â”œâ”€â”€ sgr_streaming.py
â”‚   â”œâ”€â”€ enhanced_streaming.py
â”‚   â”œâ”€â”€ sgr_visualizer.py
â”‚   â”œâ”€â”€ sgr_step_tracker.py
â”‚   â”œâ”€â”€ demo_enhanced_streaming.py
â”‚   â”œâ”€â”€ compact_streaming_example.py
â”‚   â”œâ”€â”€ scraping.py
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ reports/              # ðŸ“Š Generated reports
```

## ðŸš€ Quick Start

### Classic Version (Simple and stable)
```bash
cd sgr-classic
python sgr-deep-research.py
```

### Streaming Version (Modern with animations)
```bash
cd sgr-streaming
python sgr_streaming.py
```

Tip
- If running locally via LiteLLM + Ollama, ensure `config.yaml` points to the proxy and model alias as shown above.

## ðŸ” Version Comparison

| Feature | SGR Classic | SGR Streaming |
|---------|-------------|---------------|
| **Interface** | Simple text | Interactive with animations |
| **JSON Parsing** | Static | Real-time streaming |
| **Visualization** | Basic | Schema trees + metrics |
| **Metrics** | Simple | Detailed + performance |
| **SGR Steps** | Text log | Visual pipeline |
| **Animations** | None | Spinners, progress bars |
| **Stability** | âœ… High | âœ… Stable |
| **Simplicity** | âœ… Maximum | Medium |
| **Functionality** | Basic | âœ… Extended |

## ðŸŽ¯ Version Selection Guide

### Choose **SGR Classic** if:
- ðŸ”§ Need simple and stable system
- ðŸ’» Limited terminal resources
- ðŸ“ Focus on results, not process
- ðŸš€ Quick deployment

### Choose **SGR Streaming** if:
- ðŸŽ¨ Process visualization is important
- ðŸ“Š Need detailed metrics
- ðŸ” Want to see real-time JSON parsing
- ðŸŽ¬ Prefer modern interfaces

## âš™ï¸ General Setup

1. **Create config.yaml from example:**
```bash
cp config.yaml.example config.yaml
```

2. **Configure API keys:**
```yaml
openai:
  api_key: "your-openai-api-key"
  
tavily:
  api_key: "your-tavily-api-key"
```

3. **Install dependencies:**
```bash
# For classic version
cd sgr-classic && pip install -r requirements.txt

# For streaming version  
cd sgr-streaming && pip install -r requirements.txt
```

## ðŸŽ¬ Demo (Streaming)

```bash
cd sgr-streaming

# Full feature demonstration
python demo_enhanced_streaming.py

# Compact streaming example
python compact_streaming_example.py
```

## ðŸ“Š SGR Capabilities

### Schema-Guided Reasoning includes:
1. **ðŸ¤” Clarification** - clarifying questions when unclear
2. **ðŸ“‹ Plan Generation** - research plan creation  
3. **ðŸ” Web Search** - internet information search
4. **ðŸ”„ Plan Adaptation** - plan adaptation based on results
5. **ðŸ“ Report Creation** - detailed report creation
6. **âœ… Completion** - task completion

### Example tasks:
- "Find information about BMW X6 2025 prices in Russia"
- "Research current AI trends"
- "Analyze cryptocurrency market in 2024"

## ðŸ§  Why SGR + Structured Output?

### The Problem with Function Calling on Local Models
**Reality Check:** Function Calling works great on OpenAI/Anthropic (80+ BFCL scores) but fails on local models <32B parameters.

**Test Results:**
- `Qwen3-4B`: Only 2% accuracy in Agentic mode (BFCL benchmark)
- Local models know **HOW** to call tools, but not **WHEN** to call them
- Result: `{"tool_calls": null, "content": "Text instead of tool call"}`

### SGR Solution: Forced Reasoning â†’ Deterministic Execution

```python
# Phase 1: Structured Output reasoning (100% reliable)
reasoning = model.generate(format="json_schema")

# Phase 2: Deterministic execution (no model uncertainty)  
result = execute_plan(reasoning.actions)
```

### Architecture by Model Size

| Model Size | Recommended Approach | Why |
|------------|---------------------|-----|
| **<14B** | Pure SGR + Structured Output | FC accuracy too low, SO forces reasoning |
| **14-32B** | SGR as tool + FC hybrid | Best of both worlds |
| **32B+** | Native FC + SGR fallback | FC works reliably |

### SGR vs Function Calling

| Aspect | Traditional FC | SGR + Structured Output |
|--------|---------------|------------------------|
| **When to think** | Model decides âŒ | Always forced âœ… |
| **Reasoning quality** | Unpredictable âŒ | Structured & consistent âœ… |
| **Local model support** | <35% accuracy âŒ | 100% on simple tasks âœ… |
| **Deterministic** | No âŒ | Yes âœ… |

**Bottom Line:** Don't force <32B models to pretend they're GPT-4o. Let them think structurally through SGR, then execute deterministically.

## ðŸ”§ Configuration

### Main parameters:
```yaml
openai:
  model: "gpt-4o-mini"     # Model for reasoning
  max_tokens: 8000         # Maximum tokens
  temperature: 0.4         # Creativity (0-1)

execution:
  max_steps: 6            # Maximum SGR steps
  reports_dir: "reports"  # Reports directory
  # Optional report quality knobs (Streaming)
  strict_report_quality: false
  min_report_words: 300
  min_report_words_forced: 150

search:
  max_results: 10         # Search results count

scraping:
  enabled: false         # Web scraping
  max_pages: 5          # Maximum pages
```

## ðŸ“ Reports

All reports are saved to `reports/` directory in format:
```
YYYYMMDD_HHMMSS_Task_Name.md
```

Reports contain:
- ðŸ“‹ Executive summary
- ðŸ” Technical analysis with citations
- ðŸ“Š Key findings  
- ðŸ“Ž Sources list

## ðŸ› Fixed Issues (Streaming)

âœ… **Large gaps after streaming** - compact panels  
âœ… **Planning step duplication** - proper tracking  
âœ… **Clarification questions not displayed** - special handling  
âœ… **Schema overlapping Completed block** - proper spacing  

## ðŸ¤ Usage

Both versions are compatible and use the same configuration format. For local setups on small models (<8B), the streaming version with LiteLLM + Ollama is recommended.

Removed Modules
- The experimental `sgr_streaming_ec/` (errorâ€‘corrected streaming) module has been removed to simplify the codebase and focus on the main streaming path.

---

ðŸ§  **Choose the right SGR version for your research tasks!**
